{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "research_generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GREZYUpYDn-h",
        "colab_type": "text"
      },
      "source": [
        "# NIPS and ICML research"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rkya3TWdRxkc",
        "colab_type": "text"
      },
      "source": [
        "**Add author analysis and datasets/benchmarks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL0DGtTYDtzT",
        "colab_type": "text"
      },
      "source": [
        "## Libraries & functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6-RVEQlaR3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai import *\n",
        "from fastai.text import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPla-gYhaVF7",
        "colab_type": "code",
        "outputId": "3321b2ba-2666-483e-b9da-4c503d905528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "path = \"/content/gdrive/My Drive/Personal_projects/NeurIPS/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNJ2ab10CsHM",
        "colab_type": "code",
        "outputId": "f7ab565e-9fde-4b24-c288-fec2f7f53b7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "base_dir = root_dir + 'Personal_projects/NeurIPS/'\n",
        "dest= Path(base_dir + \"models/\")\n",
        "\n",
        "try:\n",
        "  dest.mkdir(parents=True, exist_ok=False)\n",
        "except FileExistsError:\n",
        "  print ('File Already Exists')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "File Already Exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQoyDoReq6gJ",
        "colab_type": "code",
        "outputId": "470ba974-0d41-46f2-b56d-6e9a6378270d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "#d=[]\n",
        "#while(1):\n",
        "#  d.append('1')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-29dc3913061e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8qTL1TVD1j7",
        "colab_type": "text"
      },
      "source": [
        "## Data engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twG-Ssd-ZjR-",
        "colab_type": "code",
        "outputId": "5b015cd0-17d4-4ec8-bd88-a128deacac81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df = pd.read_csv(base_dir + 'research_data_combined.csv').drop(['Unnamed: 0'],axis=1)\n",
        "print(df.shape)\n",
        "#df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12870, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDi6JL-I1uB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.abstract = df.abstract.astype(str).apply(lambda x: re.sub('<p>','',x))\n",
        "df.abstract = df.abstract.astype(str).apply(lambda x: re.sub('<div class=\"abstractContainer\">','',x))\n",
        "df.abstract = df.abstract.astype(str).apply(lambda x: re.sub('<\\/div>','',x))\n",
        "df.abstract = df.abstract.astype(str).apply(lambda x: re.sub('<\\/p>','',x))\n",
        "df.abstract = df.abstract.astype(str).apply(lambda x: re.sub('<\\/div>','',x))\n",
        "df.abstract = df.abstract.astype(str).apply(lambda x: re.sub('<br>','',x))\n",
        "df.abstract = df.abstract.astype(str).apply(lambda x: re.sub('\\n','',x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "606wx0Ce2E2F",
        "colab_type": "code",
        "outputId": "eca22f93-5a98-432f-efb6-853e2a3f7710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>author</th>\n",
              "      <th>event_type</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "      <th>conference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Update Dec 2. For the latest, click the link a...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AffinityWorkshop</td>\n",
              "      <td>Black in AI (BAI) Affinity Workshop</td>\n",
              "      <td>2019</td>\n",
              "      <td>NIPS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Despite the exploding interest in graph neural...</td>\n",
              "      <td>Aleksandar Bojchevski · Stephan Günnemann</td>\n",
              "      <td>Poster</td>\n",
              "      <td>Certifiable Robustness to Graph Perturbations</td>\n",
              "      <td>2019</td>\n",
              "      <td>NIPS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Adversarial training, in which a network is tr...</td>\n",
              "      <td>Ali Shafahi · Mahyar Najibi · Mohammad Amin Gh...</td>\n",
              "      <td>Poster</td>\n",
              "      <td>Adversarial training for free!</td>\n",
              "      <td>2019</td>\n",
              "      <td>NIPS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>An informal, inclusive event for networking an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Social</td>\n",
              "      <td>Probabilistic Programming Social</td>\n",
              "      <td>2019</td>\n",
              "      <td>NIPS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The goal of this workshop is to bring together...</td>\n",
              "      <td>Michael Mozer · Brenden Lake · Angela J Yu</td>\n",
              "      <td>Workshop</td>\n",
              "      <td>Cognitively Informed Artificial Intelligence: ...</td>\n",
              "      <td>2017</td>\n",
              "      <td>NIPS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            abstract  ... conference\n",
              "0  Update Dec 2. For the latest, click the link a...  ...       NIPS\n",
              "1  Despite the exploding interest in graph neural...  ...       NIPS\n",
              "2  Adversarial training, in which a network is tr...  ...       NIPS\n",
              "3  An informal, inclusive event for networking an...  ...       NIPS\n",
              "4  The goal of this workshop is to bring together...  ...       NIPS\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXX72b25dOXE",
        "colab_type": "code",
        "outputId": "c74cb111-a5f2-4939-f506-8e7ade9ffc81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df_filt = df[df.event_type == 'Poster']\n",
        "df_filt.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8380, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8588MhshYjX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df =  df_filt.iloc[0:6380,:]\n",
        "valid_df = df_filt.iloc[-2000:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBtH8ae1cpYx",
        "colab_type": "code",
        "outputId": "d7d39992-8509-4882-dbe4-6884eb78f7b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "data_lm = TextLMDataBunch.from_df('.',train_df=train_df, valid_df=valid_df, text_cols='abstract')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MszkghqU9e6K",
        "colab_type": "code",
        "outputId": "b0861741-5849-4c90-ea2a-fb0332f60abc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learn_lm = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://s3.amazonaws.com/fast-ai-modelzoo/wt103-fwd\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg06YxQuD9Ks",
        "colab_type": "text"
      },
      "source": [
        "## Modelling - RNNs (run once)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaNeQv80UqKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_lm.lr_find()\n",
        "learn_lm.recorder.plot(skip_end=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew_uxMOjbcw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_lm.fit_one_cycle(10, 1e-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxt1o5Ksb51P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_lm.unfreeze()\n",
        "learn_lm.lr_find()\n",
        "learn_lm.recorder.plot(skip_end=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15wgYRDLAU84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_lm.fit_one_cycle(10, 1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEG6GFJoDcDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_lm.save(dest/'final-model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giInbuoUDiQY",
        "colab_type": "text"
      },
      "source": [
        "### Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB9d5fPIDftj",
        "colab_type": "code",
        "outputId": "43409f33-0ad4-49d2-bc7f-9ef0930d4d9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learn_lm.load(dest/'final-model')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LanguageLearner(data=TextLMDataBunch;\n",
              "\n",
              "Train: LabelList (6380 items)\n",
              "x: LMTextList\n",
              "xxbos xxmaj despite the exploding interest in graph neural networks there has been little effort to verify and improve their robustness . xxmaj this is even more xxunk given recent findings showing that they are extremely vulnerable to adversarial attacks on both the graph structure and the node attributes . xxmaj we propose the first method for verifying certifiable ( xxunk to graph perturbations for a general class of models that includes graph neural networks and label / feature propagation . xxmaj by exploiting connections to pagerank and xxmaj markov decision processes our certificates can be efficiently ( and under many threat models exactly ) computed . xxmaj furthermore , we investigate robust training procedures that increase the number of certifiably robust nodes while maintaining or improving the clean predictive accuracy .,xxbos xxmaj adversarial training , in which a network is trained on adversarial examples , is one of the few defenses against adversarial attacks that xxunk strong attacks . xxmaj unfortunately , the high cost of generating strong adversarial examples makes standard adversarial training impractical on large - scale problems like imagenet . xxmaj we present an algorithm that eliminates the overhead cost of generating adversarial examples by xxunk the gradient information computed when updating model parameters . xxmaj our \" free \" adversarial training algorithm achieves comparable robustness to xxup pgd adversarial training on the xxup cifar-10 and xxup cifar-100 datasets at negligible additional cost compared to natural training , and can be 7 to 30 times faster than other strong adversarial training methods . xxmaj using a single workstation with 4 xxup xxunk gpus and 2 days of runtime , we can train a robust model for the large - scale imagenet classification task that maintains 40 % accuracy against xxup pgd attacks .,xxbos,xxbos,xxbos\n",
              "y: LMLabelList\n",
              ",,,,\n",
              "Path: .;\n",
              "\n",
              "Valid: LabelList (2000 items)\n",
              "x: LMTextList\n",
              "xxbos xxmaj transportation cost is an attractive similarity measure between probability distributions due to its many useful theoretical properties . xxmaj however , solving optimal transport exactly can be prohibitively expensive . xxmaj therefore , there has been significant effort towards the design of scalable approximation algorithms . xxmaj previous combinatorial results [ xxmaj xxunk , xxmaj xxunk xxup stoc ' 12 , xxmaj xxunk , xxmaj xxunk xxup stoc ' 14 ] have focused primarily on the design of near - linear time multiplicative approximation algorithms . xxmaj there has also been an effort to design approximate solutions with additive errors [ xxmaj xxunk xxup nips ' 13 , xxmaj xxunk \\ xxunk \\ xxup nips ' 17 , xxmaj xxunk \\ xxunk \\ , xxup icml ' 18 , xxmaj xxunk , xxup xxunk ' 19 ] within a time bound that is linear in the size of the cost matrix and polynomial in $ c / \\ delta$ ; here $ xxup c$ is the largest value in the cost matrix and $ \\ delta$ is the additive error . xxmaj we present an adaptation of the classical graph algorithm of xxmaj xxunk and xxmaj xxunk and provide a novel analysis of this algorithm that bounds its execution time by $ \\ xxunk ( \\ xxunk c } { \\ xxunk \\ xxunk } { \\ xxunk xxmaj our algorithm is extremely simple and executes , for an arbitrarily small constant $ \\ eps$ , only $ \\ xxunk \\ xxunk \\ eps ) \\ delta } \\ xxunk + 1 $ iterations , where each iteration consists only of a xxmaj xxunk - type search followed by a depth - first search . xxmaj we also provide empirical results that suggest our algorithm is competitive with respect to a sequential implementation of the xxmaj sinkhorn algorithm in execution time . xxmaj moreover , our algorithm quickly computes a solution for very small values of $ \\ delta$ whereas xxmaj sinkhorn algorithm xxunk down due to numerical instability .,xxbos xxmaj understanding why and how certain neural networks outperform others is key to guiding future development of network architectures and optimization methods . xxmaj to this end , we introduce a novel visualization algorithm that reveals the internal geometry of such networks : xxmaj xxunk xxup xxunk ( m - xxup xxunk ) , the first method designed explicitly to visualize how a neural network 's hidden representations of data evolve throughout the course of training . xxmaj we demonstrate that our visualization provides intuitive , detailed summaries of the learning dynamics beyond simple global measures ( i.e. , validation loss and accuracy ) , without the need to access validation data . xxmaj furthermore , m - xxup xxunk better captures both the dynamics and community structure of the hidden units as compared to visualization based on standard dimensionality reduction methods ( e.g. , xxup xxunk , t - xxup sne ) . xxmaj we demonstrate m - xxup xxunk with two xxunk : continual learning and generalization . xxmaj in the former , the m - xxup xxunk visualizations display the mechanism of \" catastrophic forgetting \" which is a major challenge for learning in task - switching contexts . xxmaj in the latter , our visualizations reveal how increased heterogeneity among hidden units correlates with improved generalization performance . xxmaj an implementation of m - xxup xxunk , along with xxunk to reproduce the figures in this paper , is available at https : / / github.com / xxunk / m - xxup xxunk .,xxbos xxmaj transformer architectures show significant promise for natural language processing . xxmaj given that a single pretrained model can be fine - tuned to perform well on many different tasks , these networks appear to extract generally useful linguistic features . a natural question is how such networks represent this information internally . xxmaj this paper describes qualitative and quantitative investigations of one particularly effective model , xxup bert . xxmaj at a high level , linguistic features seem to be represented in separate semantic and syntactic subspaces . xxmaj we find evidence of a fine - grained geometric representation of word senses . xxmaj we also present empirical descriptions of syntactic representations in both attention matrices and individual word embeddings , as well as a mathematical argument to explain the geometry of these representations .,xxbos xxmaj we study the geometry of deep ( neural ) networks ( xxunk ) with piecewise affine and convex nonlinearities . xxmaj the layers of such xxunk have been shown to be max - affine spline operators ( xxunk ) that partition their input space and apply a region - dependent affine mapping to their input to produce their output . xxmaj we demonstrate that each xxup xxunk layer 's input space partitioning corresponds to a power diagram ( an extension of the classical xxmaj voronoi tiling ) with a number of regions that grows exponentially with respect to the number of units ( xxunk further show that a composition of xxup xxunk layers ( e.g. , the entire xxup dn ) produces a progressively xxunk power diagram and provide its analytical form . xxmaj the xxunk process constrains the affine maps on the potentially exponentially many power diagram regions with respect to the number of neurons to greatly reduce their complexity . xxmaj for classification problems , we obtain a formula for a xxup xxunk xxup dn 's decision boundary in the input space plus a measure of its curvature that depends on the xxup dn 's nonlinearities , weights , and architecture . xxmaj numerous numerical experiments support and extend our theoretical results .,xxbos xxmaj the success of lottery xxunk initializations ( xxmaj xxunk and xxmaj xxunk , 2019 ) suggests that small , sparsified networks can be trained so long as the network is initialized appropriately . xxmaj unfortunately , finding these \" winning xxunk ' ' initializations is computationally expensive . xxmaj one potential solution is to reuse the same winning xxunk across a variety of datasets and optimizers . xxmaj however , the generality of winning xxunk initializations remains unclear . xxmaj here , we attempt to answer this question by generating winning xxunk for one training configuration ( optimizer and dataset ) and evaluating their performance on another configuration . xxmaj perhaps surprisingly , we found that , within the natural images domain , winning xxunk initializations generalized across a variety of datasets , including xxmaj fashion xxup mnist , xxup svhn , xxup cifar-10 / 100 , imagenet , and xxmaj places365 , often achieving performance close to that of winning xxunk generated on the same dataset . xxmaj moreover , winning xxunk generated using larger datasets consistently transferred better than those generated using smaller datasets . xxmaj we also found that winning xxunk initializations generalize across optimizers with high performance . xxmaj these results suggest that winning xxunk initializations generated by sufficiently large datasets contain inductive biases generic to neural networks more broadly which improve training across many settings and provide hope for the development of better initialization methods .\n",
              "y: LMLabelList\n",
              ",,,,\n",
              "Path: .;\n",
              "\n",
              "Test: None, model=SequentialRNN(\n",
              "  (0): AWD_LSTM(\n",
              "    (encoder): Embedding(13392, 400, padding_idx=1)\n",
              "    (encoder_dp): EmbeddingDropout(\n",
              "      (emb): Embedding(13392, 400, padding_idx=1)\n",
              "    )\n",
              "    (rnns): ModuleList(\n",
              "      (0): WeightDropout(\n",
              "        (module): LSTM(400, 1152, batch_first=True)\n",
              "      )\n",
              "      (1): WeightDropout(\n",
              "        (module): LSTM(1152, 1152, batch_first=True)\n",
              "      )\n",
              "      (2): WeightDropout(\n",
              "        (module): LSTM(1152, 400, batch_first=True)\n",
              "      )\n",
              "    )\n",
              "    (input_dp): RNNDropout()\n",
              "    (hidden_dps): ModuleList(\n",
              "      (0): RNNDropout()\n",
              "      (1): RNNDropout()\n",
              "      (2): RNNDropout()\n",
              "    )\n",
              "  )\n",
              "  (1): LinearDecoder(\n",
              "    (decoder): Linear(in_features=400, out_features=13392, bias=True)\n",
              "    (output_dp): RNNDropout()\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f178b7b8620>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
              "learn: LanguageLearner(data=TextLMDataBunch;\n",
              "\n",
              "Train: LabelList (6380 items)\n",
              "x: LMTextList\n",
              "xxbos xxmaj despite the exploding interest in graph neural networks there has been little effort to verify and improve their robustness . xxmaj this is even more xxunk given recent findings showing that they are extremely vulnerable to adversarial attacks on both the graph structure and the node attributes . xxmaj we propose the first method for verifying certifiable ( xxunk to graph perturbations for a general class of models that includes graph neural networks and label / feature propagation . xxmaj by exploiting connections to pagerank and xxmaj markov decision processes our certificates can be efficiently ( and under many threat models exactly ) computed . xxmaj furthermore , we investigate robust training procedures that increase the number of certifiably robust nodes while maintaining or improving the clean predictive accuracy .,xxbos xxmaj adversarial training , in which a network is trained on adversarial examples , is one of the few defenses against adversarial attacks that xxunk strong attacks . xxmaj unfortunately , the high cost of generating strong adversarial examples makes standard adversarial training impractical on large - scale problems like imagenet . xxmaj we present an algorithm that eliminates the overhead cost of generating adversarial examples by xxunk the gradient information computed when updating model parameters . xxmaj our \" free \" adversarial training algorithm achieves comparable robustness to xxup pgd adversarial training on the xxup cifar-10 and xxup cifar-100 datasets at negligible additional cost compared to natural training , and can be 7 to 30 times faster than other strong adversarial training methods . xxmaj using a single workstation with 4 xxup xxunk gpus and 2 days of runtime , we can train a robust model for the large - scale imagenet classification task that maintains 40 % accuracy against xxup pgd attacks .,xxbos,xxbos,xxbos\n",
              "y: LMLabelList\n",
              ",,,,\n",
              "Path: .;\n",
              "\n",
              "Valid: LabelList (2000 items)\n",
              "x: LMTextList\n",
              "xxbos xxmaj transportation cost is an attractive similarity measure between probability distributions due to its many useful theoretical properties . xxmaj however , solving optimal transport exactly can be prohibitively expensive . xxmaj therefore , there has been significant effort towards the design of scalable approximation algorithms . xxmaj previous combinatorial results [ xxmaj xxunk , xxmaj xxunk xxup stoc ' 12 , xxmaj xxunk , xxmaj xxunk xxup stoc ' 14 ] have focused primarily on the design of near - linear time multiplicative approximation algorithms . xxmaj there has also been an effort to design approximate solutions with additive errors [ xxmaj xxunk xxup nips ' 13 , xxmaj xxunk \\ xxunk \\ xxup nips ' 17 , xxmaj xxunk \\ xxunk \\ , xxup icml ' 18 , xxmaj xxunk , xxup xxunk ' 19 ] within a time bound that is linear in the size of the cost matrix and polynomial in $ c / \\ delta$ ; here $ xxup c$ is the largest value in the cost matrix and $ \\ delta$ is the additive error . xxmaj we present an adaptation of the classical graph algorithm of xxmaj xxunk and xxmaj xxunk and provide a novel analysis of this algorithm that bounds its execution time by $ \\ xxunk ( \\ xxunk c } { \\ xxunk \\ xxunk } { \\ xxunk xxmaj our algorithm is extremely simple and executes , for an arbitrarily small constant $ \\ eps$ , only $ \\ xxunk \\ xxunk \\ eps ) \\ delta } \\ xxunk + 1 $ iterations , where each iteration consists only of a xxmaj xxunk - type search followed by a depth - first search . xxmaj we also provide empirical results that suggest our algorithm is competitive with respect to a sequential implementation of the xxmaj sinkhorn algorithm in execution time . xxmaj moreover , our algorithm quickly computes a solution for very small values of $ \\ delta$ whereas xxmaj sinkhorn algorithm xxunk down due to numerical instability .,xxbos xxmaj understanding why and how certain neural networks outperform others is key to guiding future development of network architectures and optimization methods . xxmaj to this end , we introduce a novel visualization algorithm that reveals the internal geometry of such networks : xxmaj xxunk xxup xxunk ( m - xxup xxunk ) , the first method designed explicitly to visualize how a neural network 's hidden representations of data evolve throughout the course of training . xxmaj we demonstrate that our visualization provides intuitive , detailed summaries of the learning dynamics beyond simple global measures ( i.e. , validation loss and accuracy ) , without the need to access validation data . xxmaj furthermore , m - xxup xxunk better captures both the dynamics and community structure of the hidden units as compared to visualization based on standard dimensionality reduction methods ( e.g. , xxup xxunk , t - xxup sne ) . xxmaj we demonstrate m - xxup xxunk with two xxunk : continual learning and generalization . xxmaj in the former , the m - xxup xxunk visualizations display the mechanism of \" catastrophic forgetting \" which is a major challenge for learning in task - switching contexts . xxmaj in the latter , our visualizations reveal how increased heterogeneity among hidden units correlates with improved generalization performance . xxmaj an implementation of m - xxup xxunk , along with xxunk to reproduce the figures in this paper , is available at https : / / github.com / xxunk / m - xxup xxunk .,xxbos xxmaj transformer architectures show significant promise for natural language processing . xxmaj given that a single pretrained model can be fine - tuned to perform well on many different tasks , these networks appear to extract generally useful linguistic features . a natural question is how such networks represent this information internally . xxmaj this paper describes qualitative and quantitative investigations of one particularly effective model , xxup bert . xxmaj at a high level , linguistic features seem to be represented in separate semantic and syntactic subspaces . xxmaj we find evidence of a fine - grained geometric representation of word senses . xxmaj we also present empirical descriptions of syntactic representations in both attention matrices and individual word embeddings , as well as a mathematical argument to explain the geometry of these representations .,xxbos xxmaj we study the geometry of deep ( neural ) networks ( xxunk ) with piecewise affine and convex nonlinearities . xxmaj the layers of such xxunk have been shown to be max - affine spline operators ( xxunk ) that partition their input space and apply a region - dependent affine mapping to their input to produce their output . xxmaj we demonstrate that each xxup xxunk layer 's input space partitioning corresponds to a power diagram ( an extension of the classical xxmaj voronoi tiling ) with a number of regions that grows exponentially with respect to the number of units ( xxunk further show that a composition of xxup xxunk layers ( e.g. , the entire xxup dn ) produces a progressively xxunk power diagram and provide its analytical form . xxmaj the xxunk process constrains the affine maps on the potentially exponentially many power diagram regions with respect to the number of neurons to greatly reduce their complexity . xxmaj for classification problems , we obtain a formula for a xxup xxunk xxup dn 's decision boundary in the input space plus a measure of its curvature that depends on the xxup dn 's nonlinearities , weights , and architecture . xxmaj numerous numerical experiments support and extend our theoretical results .,xxbos xxmaj the success of lottery xxunk initializations ( xxmaj xxunk and xxmaj xxunk , 2019 ) suggests that small , sparsified networks can be trained so long as the network is initialized appropriately . xxmaj unfortunately , finding these \" winning xxunk ' ' initializations is computationally expensive . xxmaj one potential solution is to reuse the same winning xxunk across a variety of datasets and optimizers . xxmaj however , the generality of winning xxunk initializations remains unclear . xxmaj here , we attempt to answer this question by generating winning xxunk for one training configuration ( optimizer and dataset ) and evaluating their performance on another configuration . xxmaj perhaps surprisingly , we found that , within the natural images domain , winning xxunk initializations generalized across a variety of datasets , including xxmaj fashion xxup mnist , xxup svhn , xxup cifar-10 / 100 , imagenet , and xxmaj places365 , often achieving performance close to that of winning xxunk generated on the same dataset . xxmaj moreover , winning xxunk generated using larger datasets consistently transferred better than those generated using smaller datasets . xxmaj we also found that winning xxunk initializations generalize across optimizers with high performance . xxmaj these results suggest that winning xxunk initializations generated by sufficiently large datasets contain inductive biases generic to neural networks more broadly which improve training across many settings and provide hope for the development of better initialization methods .\n",
              "y: LMLabelList\n",
              ",,,,\n",
              "Path: .;\n",
              "\n",
              "Test: None, model=SequentialRNN(\n",
              "  (0): AWD_LSTM(\n",
              "    (encoder): Embedding(13392, 400, padding_idx=1)\n",
              "    (encoder_dp): EmbeddingDropout(\n",
              "      (emb): Embedding(13392, 400, padding_idx=1)\n",
              "    )\n",
              "    (rnns): ModuleList(\n",
              "      (0): WeightDropout(\n",
              "        (module): LSTM(400, 1152, batch_first=True)\n",
              "      )\n",
              "      (1): WeightDropout(\n",
              "        (module): LSTM(1152, 1152, batch_first=True)\n",
              "      )\n",
              "      (2): WeightDropout(\n",
              "        (module): LSTM(1152, 400, batch_first=True)\n",
              "      )\n",
              "    )\n",
              "    (input_dp): RNNDropout()\n",
              "    (hidden_dps): ModuleList(\n",
              "      (0): RNNDropout()\n",
              "      (1): RNNDropout()\n",
              "      (2): RNNDropout()\n",
              "    )\n",
              "  )\n",
              "  (1): LinearDecoder(\n",
              "    (decoder): Linear(in_features=400, out_features=13392, bias=True)\n",
              "    (output_dp): RNNDropout()\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f178b7b8620>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): Embedding(13392, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(13392, 400, padding_idx=1)\n",
              "  )\n",
              "  (2): LinearDecoder(\n",
              "    (decoder): Linear(in_features=400, out_features=13392, bias=True)\n",
              "    (output_dp): RNNDropout()\n",
              "  )\n",
              ")], add_time=True, silent=False)\n",
              "alpha: 2.0\n",
              "beta: 1.0], layer_groups=[Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): Embedding(13392, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(13392, 400, padding_idx=1)\n",
              "  )\n",
              "  (2): LinearDecoder(\n",
              "    (decoder): Linear(in_features=400, out_features=13392, bias=True)\n",
              "    (output_dp): RNNDropout()\n",
              "  )\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uqO2Vj-aMZG",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21JzKuTBhodg",
        "colab_type": "code",
        "outputId": "25a9e0b8-03ca-4229-d556-82d00f5c4793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "TEXT = \"We present\"\n",
        "N_WORDS = 200\n",
        "N_SENTENCES = 10\n",
        "print(\"\\n\\n\".join(learn_lm.predict(TEXT, N_WORDS, temperature=0.5) for _ in range(N_SENTENCES)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We present a novel deep neural network architecture for learning a deep network with weights that weights at high levels . The architecture consists of a recurrent network , modules that enable us to train networks using a differentiable neural network . The architecture is trained using a convolutional network , and its weights are initialized at a / - starting point at a time and the network weights become arbitrarily far from the network . The network is trained on a single image , and achieves state - of - the - art performance on several datasets . When applied to the MNIST problem on the Cityscapes problem , the network is trained with the standard backpropagation algorithm . We observe that our network achieves state - of - the - art performance on both synthetic and real - world datasets . xxbos We consider the problem of learning the structure of a Markov Chain Monte Carlo ( MCMC ) algorithm , i.e. , sampling from a posterior distribution over the parameters . We show that our algorithm is highly scalable , and can be applied to\n",
            "\n",
            "We present a novel estimator for the estimator with Gaussian mixtures . The estimator is based on the estimator with a nonlinear penalty function , and the estimator is applied to the estimation of the covariance matrix of the covariance matrix . The estimator is computationally efficient and can be used to estimate the covariance matrix of the data matrix . The estimator estimator is also a generalization of the Gibbs posterior and an unbiased estimator of the covariance matrix . We present numerical experiments to illustrate the advantage of the proposed estimator . xxbos We introduce the Deep Convolutional Neural Network ( CNN ) , a deep learning method that learns a sparse representation of the scene by selecting an appropriate representation of the input image . The QMDP - net encodes the QMDP - net on the QMDP - net and its base base . The QMDP - net encodes the QMDP - net on the QMDP - net and QMDP - net on the QMDP - net . The QMDP algorithm is also able to\n",
            "\n",
            "We present a novel algorithm for learning statistical models from a collection of unlabeled data . Our algorithm is based on a novel combination of a sequence of nested linear predictors that are computable in linear time . The algorithm is based on recursive corrections to the name of a pre - specified kernel distribution , which is based on a convex relaxation of the original problem . The algorithm is shown to be optimal in terms of the number of parameters , and can be used to efficiently estimate the number of parameters in a large number of datapoints . We show that the algorithm converges to a stationary point of the order $ O(1 / \\ epsilon)$ iterations . We also provide a theoretical analysis showing that the algorithm performs well in practice . xxbos We present a novel approach to learn a hierarchical Bayesian model for category - specific object category detection . Our model is trained to predict objects from two parts , and the resulting object candidates are learned by using a combination of the two objects . By learning the parameters of the model , we\n",
            "\n",
            "We present a general framework for learning sparse representations of data . We focus on the problem of learning representations of data using a Gaussian process prior . We present an algorithm that allows for learning a structure from data . The approach is applied to learning representations of data from a small number of latent variables . We demonstrate our method on a set of real - world problems , image segmentation , and sensor network prediction . xxbos We propose a new class of Markov chain Monte Carlo ( MCMC ) inference algorithms for general Markov chain Monte Carlo ( MCMC ) . We provide a new algorithm for MCMC sampling , the uniformization , and show that it is asymptotically efficient . The proposed algorithm is based on a Monte Carlo approximation of the posterior distribution on the proposal distribution , and we prove that it is equivalent to a Metropolis - Hastings algorithm with an approximating error of the posterior distribution on the mixing weights . We demonstrate the effectiveness of our approach on synthetic and\n",
            "\n",
            "We present a new algorithm for learning a continuous discrete pairwise graphical model with a bounded number of variables . This is a generalization of the Poisson - Gamma factor model , which is a recently proposed method for learning latent variable models . The algorithm is based on a recursive Gibbs sampling algorithm that is given an approximation of the log - partition function . We apply the algorithm to several applications : image classification , image segmentation , and image segmentation . xxbos We propose a novel approach for learning from demonstration , based on the concept of the causal Tsallis entropy . The key idea is to decouple a region into the first one , which can be used in a variety of different settings . We show that this approach is able to solve the original sub - ambiguity problem even after the goal is to be able to learn to segment a particular region ( a ) in a target region . Our method is evaluated on a real - world task of medical diagnosis and shows that our method is able to detect and segment\n",
            "\n",
            "We present a novel deep neural network architecture that allows for a variety of architecture searches . The architecture is a generalization of the popular Kalman filter that works in a recently proposed deep - learning - based architecture . The architecture is a variant of linear architecture search , and can be applied to numerous tasks including computer vision , image processing , and computer vision . To the best of our knowledge , this is the first attempt to study neural architecture search in the context of multi - task Gaussian process regression . We present a novel method , Variational Auto - Encoder , that combines the strengths of many existing methods such as Variational Bayes and Variational Inference . We use a Gaussian process prior for the function to enable the learning of the model , and enable it to learn from observed data . The proposed method is particularly effective in a wide variety of applications , including variational inference and multi - class classification . It is also shown that the proposed method can be used to learn a\n",
            "\n",
            "We present a novel algorithm for online convex optimization in the context of online learning . The algorithm is based on a Monte Carlo algorithm , and is based on a Gaussian mixture model ( CPM ) , a stochastic gradient descent algorithm ( Online Splitting ) , and a stochastic gradient descent algorithm . The algorithm is based on the Frank - Wolfe algorithm , and is agnostic to the gradient descent algorithm . We prove that the algorithm converges to a stationary point when the number of passes over the data is small , and the convergence rate is $ o ( \\ epsilon)$ , where $ d$ is the dimensionality . We also provide a simple algorithm for the strongly convex objective that is based on the gradient of the Frank - Wolfe algorithm and prove its convergence to an $ \\ ell_1 $ norm . We also show that the algorithm can be applied to a wide range of problems , including the use of Lipschitz constants in stochastic optimization . xxbos We consider the problem of learning a piecewise linear\n",
            "\n",
            "We present a novel technique for the exploration of multi - armed bandit problems . In the context of multi - armed bandits , we propose a novel algorithm , called gape - v , which combines exploration and exploitation in an online fashion . Our algorithm is based on an online algorithm , UCB - ALP , that achieves regret $ \\ tilde{o } ( \\ sqrt{t } ) , where $ T$ is time steps and $ T$ is time . In addition , we present a new algorithm , UCB - MS - UCB , which achieves improved regret bounds in both cases . We also provide algorithms that achieve $ \\ tilde{o } ( \\ sqrt{t})$ regret bound in the stochastic case , where $ T$ is the time horizon . xxbos We consider the problem of learning a Markov Decision Process ( MDP ) for a given environment . We show that the optimal policy can be represented as a Markov decision process ( MDP ) , with a reward defined on a Mdp .\n",
            "\n",
            "We present a probabilistic model for a category that explains a wide variety of aspects in the context of object recognition . First , we learn a category for category - level category category recognition by attempting to learn a category from a single category of categories by drawing upon category geometry from category - specific perspectives . Second , we learn category - specific category features from category - level category - level features in category - level features . We then introduce a hierarchical latent variable model for category category - specific category category category learning . Category - level features are extracted using a number of category - specific features to represent the category 's category . In each category , category features are extracted from the category - level features . We achieve this by jointly estimating category - specific features and category - specific features . The category features are constructed by transforming the image into a compact patch space and jointly minimizing the feature maps of the category 's category . Experimental results show that our category - level features outperform object category recognition . xxbos We\n",
            "\n",
            "We present a novel algorithm for learning the binary vector of binary binary codes . The algorithm is based on a Monte - Carlo Tree Search ( MCTS ) algorithm , and is trained using a Monte Carlo algorithm . The algorithm is motivated by the fact that the binary codes of a binary binary spike counts are more informative about the target . We show how this algorithm can be used to construct a binary classification model using a binary classification problem . Our algorithm is a generalization of the Kalman filter to the case where the binary codes of the binary codes are obtained by minimizing the log - loss . We show that the algorithm is equivalent to the binary codes of binary codes which are currently available at : https : / / github.com / Shen - Lab / SVM . xxbos We study the problem of learning the structure of a discrete graphical model from a data point , where the number of observations is the number of variables in the graph . This problem arises frequently in many\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vs6945O9ziS",
        "colab_type": "code",
        "outputId": "c10e0175-b335-45cb-cf64-48548b0607b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "TEXT = \"Research\"\n",
        "N_WORDS = 200\n",
        "N_SENTENCES = 10\n",
        "print(\"\\n\\n\".join(learn_lm.predict(TEXT, N_WORDS, temperature=0.7) for _ in range(N_SENTENCES)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Research in machine learning has raised concerns over the recent advances in deep learning . In this paper we investigate how the problem of continual learning can be solved by learning a discrete mapping function from a data point . We introduce two novel regularization schemes , which we call q - learning , that we call pc - bo(nested ) . We first show that pc - bo(nested ) is an online learning algorithm based on the sketching framework . We then present an efficient algorithm for posterior inference under nystrom assumptions and provide a powerful inference algorithm based on Nystrom approximation of the posterior distribution over nystrom and the learning of posterior parameters with Nystrom approximation . We evaluate our algorithm on the Nystrom approximation of nystrom , in which it is able to develop an efficient Nystrom approximation to nystrom and empirically demonstrate its efficiency on a Nystrom approximation . Furthermore , we show that the method is more accurate than its NYSTROM counterpart and permits a wide variety of applications in both continuous and categorical domains . xxbos We study the problem of recovering\n",
            "\n",
            "Research on the processes of past causality have focused on developing a novel approach to detecting and analyzing the underlying popularity of deep learning models with the stability of some underlying dynamics . However , this is not done in the classical framework of Bayesian Online Learning ( CBS ) , which is highly difficult to consider in man and other real - world networks . In this work , we introduce a new framework for learning a hierarchical system that learns a probabilistic mathematical model of what is done by integrating a Bayesian network with a single hidden neuron function . We show that the model contains a simple Bayesian network that takes into account both cortical connectivity and model parameters in its supervised manner . We demonstrate our method on the task of image denoising and perceptual inference . xxbos We present a novel approach to deep neural networks , which relies on the integration of a neural network into a two - layer linear model . This architecture is based on a recently proposed network architecture called the Convolutional Neural Network (\n",
            "\n",
            "Research in machine learning has been impeded by the lack of labeled data . Recent work has proposes an attention mechanism for image captioning . In this paper , we propose a new model of image captioning inspired by the Conditional Graph Convolution Network ( GCN ) . We introduce a hierarchical Bernoulli model for the attention mechanism that allows the agent to focus on the one - step learning of the target network , as well as a discriminator that we learn to convey . The network learns to generate a set of pixels from a large collection of images , and we derive an algorithm that is able to learn the model itself . The encoder and decoder can be trained jointly to preserve the global and local information of the input images . We also define a two - stage model for the image - to - image translation task . Our model performs search over the image - level . Extensive experiments on both synthetic and real datasets verify the effectiveness of our model . In particular , our model contains a\n",
            "\n",
            "Research on animal learning , such as Bayesian inference and Bayesian inference , has been actively studied for decades . In this paper , we aim to diversify the underlying model and exploit recent advances in statistical physics . We propose a framework that connects structural assumptions on the model in which a system consists of very different models , where knowledge of how the animal 's behavior changes from the environment depends on the meaning of its observed components . Our method is based on a Bayesian model , with a prior over model parameters , which we call Probabilistic Model Temporal Animal ( DESPOT ) . The DESPOT is then fitted to a Markov decision process using a POMDP model , and estimates the parameters of the DESPOT as a function of the DESPOT . We show that under the CIRCUMSTANCES , the CIRCUMSTANCES favoring model - based RL can be uniquely measured via Markov Decision Processes , making this assumption possible . xxbos We study the problem of online learning for online learning\n",
            "\n",
            "Research on human perception and human cognition has been an unsolved problem in the data - driven framework of human perception . In this paper , we introduce an approach to learning human abstract reasoning from a human ’s inferences about the intent of other students , such as human or cognitive . In our framework , human actions become increasingly relevant for answering an open question , where the babi dataset is stored and actions are assigned to task - specific environments . We introduce a new concept called self - attention , called Spatial Attention , that generalizes much more recently . We define a Bayesian model of human subjects , and show that VQA exhibits biases that were recently developed for animal learnability . We show that , under weak assumptions on the model , learning stability and batch learning approaches can be achieved , while maintaining a power of linear and linear dynamics . The transformed concept and model is publicly available at https : / / github.com / Microsoft / edgeml / tree / master / software / CLEVR . xxbos We address\n",
            "\n",
            "Research on human visual attention ( lfo ) has been widely studied in that it has been optimizing a complex , non - smooth , and practically relevant objective function . Here , we take a step towards this goal and propose novel mechanisms to solve this problem . We focus on the problem of spatio - temporal action detection , where the goal is to localize the entire action in a video while the action is incrementally segmentation . In contrast to previous work in video synthesis , the goal is to learn the action of such video by mapping the video to the video space . We cast the problem as a design problem , and present an algorithm that learns this representation from observation of the action video . Our system learns to segment from the video ( video ) and frames recent advances in structural 3D video synthesis and most action potential generation . The videos produced by our algorithm are evaluated on a challenging semantic segmentation task . The experimental results show that our method outperforms strong baselines on a number of tasks and has comparable performance to\n",
            "\n",
            "Research on this field has primarily focused on developing efficient algorithms for building large scale classifiers . Existing approaches mainly focus on learning a classifier from unlabeled data and some efforts to learn search heuristics that rely on manually annotated training examples . In this paper , we introduce a new approach to designing classifiers for this problem . Our method is based on an adversarial training method , which iteratively refines the training using unlabeled data . Our method is based on a generalized gradient descent algorithm that does not require training or validation time to be annotated . We demonstrate the robustness and robustness of our method on real - world datasets using a dataset of handwritten digits , Web photos , and Omniglot . Our method is the first to achieve high accuracy on imagenet , and in slightly reduced precision . Our algorithm is much faster and can be seen as a more efficient method for training deep neural networks . xxbos We consider the problem of estimating the mean of a function $ \\ Theta ( \\ ln n)$ in a Hilbert space $\n",
            "\n",
            "Research on neural networks has primarily focused on modeling the local computation of neural networks and computing probabilistic models on devices . Here we address the problem of inferring the hidden state of a neural network from noisy observations of its inputs . We show that , in contrast to the deep learning models , that have been recently proposed as a nonlinear function of the latent variable models , the critical problem in the PAC - Bayesian theory is not at least as hard as the SPN , if the observed data is missing , but rather on a given observation . We demonstrate that the neural network can learn to \" denoise \" the network , and that this network is able to find a \" small \" fraction of network parameters . We also propose an algorithm that takes advantage of the implicit dynamics of the cascades to start and start training . xxbos We introduce a new approach to the problem of efficient and generalizable Bayesian inference in Bayesian networks . In particular , we consider the problem of learning the structure of a probabilistic graphical\n",
            "\n",
            "Research on human visual processing has mostly been focussed on the development of an automated feed - forward neural network ( SNN ) for learning visual representations . Given a demonstration of this task , GNN is able to predict a diverse set of visual behaviors , where each task is formulated as a risk over a universal representation of the visual features . In this paper , we propose a framework for unsupervised learning of visual representations . The key feature of GNN is to find what is better than previously known , computational and neurobiological representations that can be used to model the visual information geometry of visual cortex . We propose a novel representation of visual attention , called 3D object Oriented Question Answering ( VQA ) , for stitching the domain knowledge . Given a scene consisting of k - VQA and two - billion - edge sampling , VQA - CP is trained on the VQA dataset to learn visual concepts that are most representative of the visual recognition tasks , and generates a new answer by exploiting the\n",
            "\n",
            "Research on neural networks has been an important research topic in recent years . However , despite their great successes , translating state - of - art neural networks have a very limited capacity to capture the fact that neural networks are vulnerable to adversarial attacks . In this paper , we propose a new deep neural network architecture that is simple and robust to algorithmic difficulties . We develop a novel algorithm , SAGA , that recursively corrects for the first time an attack - by - training algorithm . The algorithm is tested on a MNIST dataset with over 10,000 parameters . We compare SEM to its state - of - the - art predictive performance , and show that it outperforms existing methods . We apply it to the Handwritten digit recognition dataset , where our model outperformed all the previous methods in terms of accuracy , and is able to find the single CIFAR-10 classifier with respect to the data - generating net . xxbos We consider the problem of recovering a piecewise linear function from a sparse set of measurements given only a subset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmiPB_QqQpWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.options.display.max_colwidth = 5000\n",
        "pd.set_option('display.max_rows', 100)\n",
        "df.sample(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSW2LLfRROJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}